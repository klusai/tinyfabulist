## Evaluation Averages
| Model | Grammar | Creativity | Moral Clarity | Adherence to Prompt | Count |
|-------|---------|------------|--------------|---------------------|-------|
| Llama 3.1 8B Instruct | 8.47 | 6.59 | 7.77 | 8.42 | 100 |
| DeepSeek R1 Distill Llama 8B | 6.98 | 5.75 | 6.82 | 5.85 | 100 |
| SmolLM2-1.7B-Instruct | 6.87 | 5.07 | 6.59 | 5.56 | 100 |
| Llama-3.2-1B-Instruct | 6.79 | 5.12 | 6.23 | 5.49 | 100 |
| Mistral-7B-Instruct-v0.3   | 8.53 | 6.58 | 8.02 | 8.15 | 100 |
