## Translation Evaluation Averages

| Model | Translation Accuracy | Fluency | Moral Clarity | Average Score (Mean) | Count | Avg Input Tokens | Avg Output Tokens | Avg Inference Time (s) |
|-------|----------------------|---------|---------------|----------------------|-------|------------------|-------------------|------------------------|

| gemma3-4b-2iterImproved | 8.42 | 8.67 | 9.29 | 8.79 | 160 | 181.6 | 337.4 | 52.97 |
| EuroLLM | 8.71 | 8.61 | 9.43 | 8.92 | 491 | 0.0 | 0.0 | 0.00 |
| Llama-3.1-Tulu-3-8B_gpt-4.1-mini-2025-04-14 | 8.36 | 8.38 | 9.66 | 8.80 | 100 | 181.5 | 368.5 | 16.91 |
| Llama-3.1-8B-Instruct | 7.45 | 7.80 | 8.74 | 8.00 | 200 | 181.7 | 337.5 | 92.70 |
| gemma3-27b | 8.97 | 8.97 | 9.62 | 9.19 | 140 | 181.7 | 337.8 | 50.70 |
| gemma3-4b | 8.41 | 8.59 | 9.35 | 8.78 | 159 | 181.7 | 337.4 | 52.04 |
| Llama-3.1-8B-Instruct-Iterational | 7.20 | 7.42 | 8.48 | 7.70 | 115 | 181.9 | 337.5 | 75.15 |
| gemma3-4b-2iter | 8.39 | 8.76 | 9.33 | 8.82 | 160 | 181.6 | 337.4 | 52.81 |
| Gemma3 | 8.88 | 8.86 | 9.60 | 9.11 | 108 | 181.8 | 337.7 | 58.07 |
| Llama-3.1-Tulu-3-8B_gpt-4.1-nano-2025-04-14 | 6.87 | 6.84 | 8.72 | 7.48 | 100 | 181.5 | 368.5 | 16.91 |
| Llama-3.1-Tulu-3-8B_gpt-4.1-2025-04-14 | 8.90 | 8.80 | 9.71 | 9.14 | 100 | 181.5 | 368.5 | 16.91 |
| Deepl | 8.14 | 8.37 | 8.89 | 8.47 | 100 | 141.5 | 350.6 | 101.69 |
| Gpt-4o | 8.64 | 8.55 | 9.24 | 8.81 | 100 | 141.5 | 350.6 | 101.69 |