## Evaluation Averages

| Model | Grammar | Creativity | Moral Clarity | Adherence to Prompt | Average Score (Mean) | Count | Avg Input Tokens | Avg Output Tokens | Avg Inference Time (s) |
|-------|---------|------------|---------------|---------------------|-----------------|-------|-----------------|------------------|------------------------|
| Llama-3.1-8B-Instruct_gpt-4o | 8.51 | 7.58 | 8.64 | 8.20 | 8.23 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_gpt-o3-mini | 8.44 | 7.57 | 8.52 | 8.18 | 8.18 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_llama-3-1-8b | 7.09 | 6.48 | 7.42 | 6.02 | 6.75 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_deepl | 8.29 | 7.40 | 8.46 | 8.09 | 8.06 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct Mbart-large-en-ro | 5.24 | 5.48 | 5.81 | 4.63 | 5.29 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct-pansophic | 5.57 | 4.41 | 5.49 | 3.87 | 4.83 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_nllb-200-3.3b | 7.06 | 5.62 | 3.56 | 3.40 | 4.91 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_Mistral-7B | 5.49 | 6.15 | 6.78 | 5.69 | 6.03 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_DeepSeek-R1-Distill-Llama-70B | 7.73 | 6.70 | 6.54 | 6.17 | 6.79 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_Llama-3.3-70B | 8.22 | 7.35 | 8.48 | 7.77 | 7.96 | 100 | 141.5 | 350.6 | 101.69 |
