## Translation Evaluation Averages

| Model | Translation Accuracy | Fluency | Style Preservation | Moral Clarity | Average Score (Mean) | Count | Avg Input Tokens | Avg Output Tokens | Avg Inference Time (s) |
|-------|---------------------|---------|-------------------|---------------|-----------------|-------|-----------------|------------------|------------------------|
| Llama-3.1-8B-Instruct_deepl | 8.14 | 8.37 | 8.23 | 8.89 | 8.41 | 100 | 141.5 | 350.6 | 101.69 |
| _Enhanced-Llama-3.3-70B | 8.29 | 8.29 | 8.29 | 8.92 | 8.45 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_gpt-4o | 8.64 | 8.55 | 8.62 | 9.24 | 8.76 | 100 | 141.5 | 350.6 | 101.69 |
| Llama-3.1-8B-Instruct_Llama-3.3-70B-Instruct | 7.52 | 7.83 | 7.67 | 8.67 | 7.92 | 100 | 141.5 | 350.6 | 101.69 |
| _Enhanced-Llama-3.3-70B_Fine_Prompted | 8.18 | 8.29 | 8.30 | 8.95 | 8.43 | 100 | 141.5 | 350.6 | 101.69 |
| _Enhanced-Llama-3.3-70B_Fine_Prompted-2 | 8.07 | 8.21 | 8.26 | 8.98 | 8.38 | 100 | 141.5 | 350.6 | 101.69 |
| _Enhanced-Llama-3.3-70B_Fine_Prompted-3 | 7.90 | 8.24 | 8.18 | 8.98 | 8.32 | 100 | 141.5 | 350.6 | 101.69 |
| _Enhanced-Llama-3.3-70B_Fine_Prompted-4 | 8.11 | 8.39 | 8.37 | 9.06 | 8.48 | 100 | 141.5 | 350.6 | 101.69 |
