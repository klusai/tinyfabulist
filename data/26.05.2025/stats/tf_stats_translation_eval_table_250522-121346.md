## Translation Evaluation Averages

| Model | Translation Accuracy | Fluency | Style Preservation | Moral Clarity | Average Score (Mean) | Count | Avg Input Tokens | Avg Output Tokens | Avg Inference Time (s) |
|-------|----------------------|---------|--------------------|---------------|----------------------|-------|------------------|-------------------|------------------------|

| gemma3-4b-2iterImproved | 8.42 | 8.67 | 7.71 | 9.29 | 8.52 | 160 | 181.6 | 337.4 | 52.97 |
| EuroLLM | 8.71 | 8.61 | 7.90 | 9.43 | 8.66 | 491 | 0.0 | 0.0 | 0.00 |
| Llama-3.1-Tulu-3-8B_gpt-4.1-mini-2025-04-14 | 8.36 | 8.38 | 8.58 | 9.66 | 8.75 | 100 | 181.5 | 368.5 | 16.91 |
| Llama-3.1-8B-Instruct | 7.45 | 7.80 | 7.06 | 8.74 | 7.76 | 200 | 181.7 | 337.5 | 92.70 |
| gemma3-27b | 8.97 | 8.97 | 8.12 | 9.62 | 8.92 | 140 | 181.7 | 337.8 | 50.70 |
| gemma3-4b | 8.41 | 8.59 | 7.80 | 9.35 | 8.54 | 159 | 181.7 | 337.4 | 52.04 |
| Llama-3.1-8B-Instruct-Iterational | 7.20 | 7.42 | 6.65 | 8.48 | 7.44 | 115 | 181.9 | 337.5 | 75.15 |
| gemma3-4b-2iter | 8.39 | 8.76 | 7.78 | 9.33 | 8.56 | 160 | 181.6 | 337.4 | 52.81 |
| Gemma3-2Iter | 8.82 | 8.85 | 8.02 | 9.33 | 8.75 | 61 | 181.3 | 338.8 | 57.60 |
| Gemma3 | 8.88 | 8.86 | 8.06 | 9.60 | 8.85 | 108 | 181.8 | 337.7 | 58.07 |
| Llama-3.1-Tulu-3-8B_gpt-4.1-nano-2025-04-14 | 6.87 | 6.84 | 7.24 | 8.72 | 7.42 | 100 | 181.5 | 368.5 | 16.91 |
| Llama-3.1-Tulu-3-8B_gpt-4.1-2025-04-14 | 8.90 | 8.80 | 8.92 | 9.71 | 9.08 | 100 | 181.5 | 368.5 | 16.91 |
| Deepl | 8.14 | 8.37 | 8.23 | 8.89 | 8.41 | 100 | 141.5 | 350.6 | 101.69 |
| Gemma3-U2iter | 8.89 | 8.99 | 8.12 | 9.59 | 8.90 | 94 | 181.9 | 338.4 | 42.21 |
| Gpt-4o | 8.64 | 8.55 | 8.62 | 9.24 | 8.76 | 100 | 141.5 | 350.6 | 101.69 |