
@article{nitu_natural_2024,
	title = {Natural Language Processing Tools for Romanian – Going Beyond a Low-Resource Language.},
	issn = {22832998},
	url = {https://ixdea.org/60_SP_1/},
	doi = {10.55612/s-5002-060-001sp},
	abstract = {Advances in Natural Language Processing bring innovative instruments to the educational field to improve the quality of the didactic process by addressing challenges like language barriers and creating personalized learning experiences. Most research in the domain is dedicated to high-resource languages, such as English, while languages with limited coverage, like Romanian, are still underrepresented in the field. Operating on low-resource languages is essential to ensure equitable access to educational opportunities and to preserve linguistic diversity. Through continuous investments in developing Romanian educational instruments, we are rapidly going beyond a low-resource language. This paper presents recent educational instruments and frameworks dedicated to Romanian, leveraging state-of-the-art {NLP} techniques, such as building advanced Romanian language models and benchmarks encompassing tools for language learning, text comprehension, question answering, automatic essay scoring, and information retrieval. The methods and insights gained are transferable to other low-resource languages, emphasizing methodological adaptability, collaborative frameworks, and technology transfer to address similar challenges in diverse linguistic contexts. Two use cases are presented, focusing on assessing student performance in Moodle courses and extracting main ideas from students’ feedback. These practical applications in Romanian academic settings serve as examples for enhancing educational practices in other less-resourced languages.},
	pages = {7--26},
	number = {60},
	journaltitle = {Interaction Design and Architecture(s)},
	shortjournal = {{IxD}\&A},
	author = {Nitu, Melania and Dascalu, Mihai},
	urldate = {2025-01-24},
	date = {2024-03-15},
	langid = {english},
}

@misc{gunasekar_textbooks_2023,
	title = {Textbooks Are All You Need},
	url = {http://arxiv.org/abs/2306.11644},
	doi = {10.48550/arXiv.2306.11644},
	abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with {GPT}-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on {HumanEval} and 55.5\% on {MBPP}. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on {HumanEval}.},
	number = {{arXiv}:2306.11644},
	publisher = {{arXiv}},
	author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Giorno, Allie Del and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and Rosa, Gustavo de and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
	urldate = {2025-01-24},
	date = {2023-10-02},
	eprinttype = {arxiv},
	eprint = {2306.11644 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}
