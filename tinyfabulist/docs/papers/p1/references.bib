
@article{qian_understanding_2023,
	title = {Understanding Chinese Moral Stories with Further Pre-Training},
	volume = {12},
	issn = {23194111},
	url = {https://aircconline.com/ijnlc/V12N2/12223ijnlc01.pdf},
	doi = {10.5121/ijnlc.2023.12201},
	abstract = {The goal of moral understanding is to grasp the theoretical concepts embedded in a narrative by delving beyond the concrete occurrences and dynamic personas. Specifically, the narrative is compacted into a single statement without involving any characters within the original text, necessitating a more astute language model that can comprehend connotative morality and exhibit commonsense reasoning. The “pretraining + fine-tuning” paradigm is widely embraced in neural language models. In this paper, we propose an intermediary phase to establish an improved paradigm of “pre-training + further pre-training + finetuning”. Further pre-training generally refers to continual learning on task-specific or domain-relevant corpora before being applied to target tasks, which aims at bridging the gap in data distribution between the phases of pre-training and fine-tuning. Our work is based on a Chinese dataset named {STORAL}-{ZH} that composes of 4k human-written story-moral pairs. Furthermore, we design a two-step process of domain-adaptive pre-training in the intermediary phase. The first step depends on a newly-collected Chinese dataset of Confucian moral culture. And the second step bases on the Chinese version of a frequently-used commonsense knowledge graph (i.e. {ATOMIC}) to enrich the backbone model with inferential knowledge besides morality. By comparison with several advanced models including {BERTbase}, {RoBERTa}-base and T5-base, experimental results on two understanding tasks demonstrate the effectiveness of our proposed three-phase paradigm.},
	pages = {01--12},
	number = {2},
	journaltitle = {International Journal on Natural Language Computing},
	shortjournal = {{IJNLC}},
	author = {Qian, Jing and Yue, Yong and Atkinson, Katie and Li, Gangmin},
	urldate = {2025-01-26},
	date = {2023-04-29},
	langid = {english},
	keywords = {Chinese moral stories},
}

@article{nitu_natural_2024,
	title = {Natural Language Processing Tools for Romanian – Going Beyond a Low-Resource Language.},
	issn = {22832998},
	url = {https://ixdea.org/60_SP_1/},
	doi = {10.55612/s-5002-060-001sp},
	abstract = {Advances in Natural Language Processing bring innovative instruments to the educational field to improve the quality of the didactic process by addressing challenges like language barriers and creating personalized learning experiences. Most research in the domain is dedicated to high-resource languages, such as English, while languages with limited coverage, like Romanian, are still underrepresented in the field. Operating on low-resource languages is essential to ensure equitable access to educational opportunities and to preserve linguistic diversity. Through continuous investments in developing Romanian educational instruments, we are rapidly going beyond a low-resource language. This paper presents recent educational instruments and frameworks dedicated to Romanian, leveraging state-of-the-art {NLP} techniques, such as building advanced Romanian language models and benchmarks encompassing tools for language learning, text comprehension, question answering, automatic essay scoring, and information retrieval. The methods and insights gained are transferable to other low-resource languages, emphasizing methodological adaptability, collaborative frameworks, and technology transfer to address similar challenges in diverse linguistic contexts. Two use cases are presented, focusing on assessing student performance in Moodle courses and extracting main ideas from students’ feedback. These practical applications in Romanian academic settings serve as examples for enhancing educational practices in other less-resourced languages.},
	pages = {7--26},
	number = {60},
	journaltitle = {Interaction Design and Architecture(s)},
	shortjournal = {{IxD}\&A},
	author = {Nitu, Melania and Dascalu, Mihai},
	urldate = {2025-01-26},
	date = {2024-03-15},
	langid = {english},
	keywords = {romanian corpora},
}

@misc{gunasekar_textbooks_2023,
	title = {Textbooks Are All You Need},
	url = {http://arxiv.org/abs/2306.11644},
	doi = {10.48550/arXiv.2306.11644},
	abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with {GPT}-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on {HumanEval} and 55.5\% on {MBPP}. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on {HumanEval}.},
	number = {{arXiv}:2306.11644},
	publisher = {{arXiv}},
	author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Giorno, Allie Del and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and Rosa, Gustavo de and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
	urldate = {2025-01-24},
	date = {2023-10-02},
	eprinttype = {arxiv},
	eprint = {2306.11644 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}
