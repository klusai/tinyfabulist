% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\title{TinyFabulist: A Romanian-English Fable Dataset and Lightweight Model for Low-Resource NLP}

\author{Mihai Nadăș \and Laura Dioșan \\
  Babeș-Bolyai University \\
  Cluj-Napoca, Romania \\
  \texttt{\{mihai.nadas, laura.diosan\}} \\
  \texttt{@ubbcluj.ro} \\\And
  Andreea Tomescu \\
  KlusAI \\
  \texttt{andreea.tomescu@klusai.com} \\}

\begin{document}
\maketitle
\begin{abstract}
We introduce \textbf{kai/ds-tfab1}, a synthetic text dataset comprising over 2.5 million fables in Romanian and English, and \textbf{kai/tfab1-1M}, a transformer-based, encoder-only language model pre-trained on this dataset. \textbf{kai/ds-tfab1} represents, at the time of this study, the largest dataset of English-Romanian fable pairs, filling a critical gap in resources for low-resource languages. We demonstrate that high-quality data generation for low-resource languages is feasible using large language models (LLMs) on consumer-grade GPUs. Additionally, we show that lightweight models with as few as 1 million parameters can achieve strong NLP task performance on resource-constrained devices like Raspberry Pi. These findings pave the way for task-specific, low-resource NLP applications on constrained devices.
\end{abstract}

\section{Introduction (1 Page)}
\begin{enumerate}
    \item Why are low-resource languages underrepresented in NLP, and why is synthetic data important?  
    \textit{Discuss data scarcity challenges in low-resource languages, emphasizing the need for innovative datasets to democratize NLP. Highlight fables as an ideal format for synthetic dataset generation.}

    \item What are the contributions of this paper, and how do they advance NLP research?  
    \textit{List key contributions, such as creating the largest bilingual Romanian-English fable dataset, introducing a lightweight encoder-only model, and demonstrating feasibility on constrained devices.}

    \item Why are fables an ideal format for synthetic dataset generation, and what unique advantages do they offer for training NLP models?  
    \textit{Explain the simplicity, moral-driven structure, and universality of fables, which make them ideal for NLP tasks like reasoning, summarization, and cross-lingual learning. Include examples of how fables naturally align with combinatorial and generative methodologies. \citep{qian_understanding_2023}}

    \item What is the scope of this paper?  
    \textit{Limit the focus to bilingual Romanian-English datasets and models, avoiding overgeneralization to other languages or domains.}
\end{enumerate}

\section{Related Work (0.75 Pages)}
\begin{enumerate}
    \item What synthetic and multilingual datasets currently exist, and how do they compare to \textbf{kai/ds-tfab1}?  
    \textit{Summarize existing datasets (?), focusing on gaps in bilingual narrative synthesis. Highlight the novelty of fables for creative NLP tasks. \citep{nitu_natural_2024}}

    \item What lightweight models have been proposed for constrained devices, and how does \textbf{kai/tfab1-1M} compare?  
    \textit{Review similar models (?), discussing trade-offs and the benefits of your encoder-only architecture.}

    \item What are the ethical implications of synthetic datasets, and how does this work address them?  
    \textit{Briefly discuss potential biases in synthetic data and the importance of quality control mechanisms.}
\end{enumerate}

\section{Dataset Creation and Validation (1.25 Pages)}
\subsection{Data Generation Pipeline}
\begin{enumerate}
    \item What models and methods were used to generate the dataset?  
    \textit{Justify the use of LLMs for bilingual fable generation, highlighting accessibility and performance on narrative tasks.}

    \item How were prompts and sampling strategies designed to ensure diversity?  
    \textit{Describe prompt engineering, nucleus sampling, and other strategies to balance creativity and coherence. Include an example.}

    \item What quality control processes ensured dataset accuracy and relevance?  
    \textit{Outline automated and human validation steps, combining perplexity scores with manual reviews.}
\end{enumerate}

\subsection{Dataset Statistics}
\begin{enumerate}
    \item How large and diverse is \textbf{kai/ds-tfab1}?  
    \textit{Provide descriptive statistics, such as total size, average story length, lexical diversity, and thematic breadth.}

    \item How does this dataset compare to existing bilingual or narrative datasets?  
    \textit{Use comparative metrics like narrative complexity and thematic variety to substantiate claims.}
\end{enumerate}

\section{Model Development and Training (1 Page)}
\begin{enumerate}
    \item Why was an encoder-only architecture chosen for \textbf{kai/tfab1-1M}?  
    \textit{Explain its suitability for efficiency and resource-constrained devices, comparing trade-offs with encoder-decoder models.}

    \item How was the model trained, and what resources were used?  
    \textit{Summarize training datasets, hyperparameters, hardware, and optimization strategies. Include reproducibility considerations.}

    \item How does the model’s design and size affect deployment feasibility?  
    \textit{Discuss the advantages of the lightweight architecture, particularly on devices like Raspberry Pi.}
\end{enumerate}

\section{Evaluation and Results (2 Pages)}
\subsection{Dataset Evaluation}
\begin{enumerate}
    \item How was linguistic and thematic diversity evaluated in \textbf{kai/ds-tfab1}?  
    \textit{Use metrics like type-token ratio, ROUGE, and narrative coherence to assess quality.}

    \item How does this dataset perform compared to existing bilingual datasets?  
    \textit{Highlight strengths in linguistic richness and thematic coverage.}
\end{enumerate}

\subsection{Model Evaluation}
\begin{enumerate}
    \item How does \textbf{kai/tfab1-1M} perform on NLP tasks like summarization, translation, and sentiment analysis?  
    \textit{Compare performance to lightweight models like DistilBERT, using task-specific metrics.}

    \item How does the model perform on resource-constrained devices?  
    \textit{Provide benchmarks for latency, memory usage, and energy efficiency, particularly for Raspberry Pi.}
\end{enumerate}

\section{Discussion and Limitations (0.75 Pages)}
\begin{enumerate}
    \item What are the key insights from this study, and how do they advance NLP for low-resource languages?  
    \textit{Summarize findings and practical implications for constrained-device NLP.}

    \item What limitations should readers be aware of?  
    \textit{Acknowledge limitations in dataset generalizability, potential biases, and model scalability. Suggest areas for future research.}
\end{enumerate}

\section{Conclusion (0.25 Pages)}
\begin{enumerate}
    \item What are the main contributions of this paper?  
    \textit{Reiterate the creation of \textbf{kai/ds-tfab1}, the introduction of \textbf{kai/tfab1-1M}, and their feasibility for constrained devices.}

    \item What is the potential impact of this work on NLP research and applications?  
    \textit{Encourage further development of low-resource NLP tools and datasets, calling for collaboration and adoption.}
\end{enumerate}

\bibliography{references}

\appendix
\section{Appendix (Optional, Excluded from Page Count)}
\begin{itemize}
    \item Include dataset samples (e.g., Romanian-English fables).
    \item Provide detailed training configurations, prompts, and additional examples for reproducibility.
\end{itemize}

\end{document}