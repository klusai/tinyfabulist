## Model Evaluation on English and Romanian Fables

We conducted an evaluation of 5 language models using a dataset of fables. The results are shown in **Picture 1**, summarized in the markdown table below.

Clean Deepseek = Parsed deepseek, without thinking tags.

> ðŸ§  Notably, **Mistral 7B** and **LLaMA 8B** performed very closely in terms of quality and accuracy.

---

### Step 2: Evaluation on Romanian Translations

In **Picture 2**, we re-evaluate the models using a **Romanian-translated version of the same fables**, to ensure consistency across languages. This is an important step as the dataset will also be made available in **Romanian** for broader accessibility and research use.